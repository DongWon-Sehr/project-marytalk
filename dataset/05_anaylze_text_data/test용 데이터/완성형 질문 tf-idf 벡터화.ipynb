{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 만들어진 질문 데이터 - train/test 데이터로 나누기\n",
    "- train/test 데이터를 총 3가지 종류로 나눠보려 함\n",
    "- 1) 질문의 의도만을 파악(추천/ 문의(가격, 장소, 리뷰) 인지\n",
    "- 2) 어떤 특성(ex) 잘하는 곳인지 ,꼼꼼한지, 깨끗한 곳인지...) + 어떤 헤어스타일인지\n",
    "- 3) 질문의 의도 + 어떤 특성 + 어떤 헤어스타일인지\n",
    "- ex) 마지막까지 챙겨주는 댄디컷 솜씨좋은 헤어샵 어디있어요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from konlpy.tag import Mecab\n",
    "import platform\n",
    "path = \"c:/Windows/Fonts/malgun.ttf\" \n",
    "from matplotlib import font_manager, rc \n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family='AppleGothic') \n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc('font', family=font_name)\n",
    "else:\n",
    "    print('Unknown system... sorry~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rainb\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>question_mecab</th>\n",
       "      <th>tag_intention</th>\n",
       "      <th>tag_adjs</th>\n",
       "      <th>tag_hair</th>\n",
       "      <th>all_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4567387</th>\n",
       "      <td>아뜰리에7 스핀스왈로펌 비용 어떻지?</td>\n",
       "      <td>뜰 리 스핀 스 왈 로펌 비용 어떻</td>\n",
       "      <td>price</td>\n",
       "      <td>NaN</td>\n",
       "      <td>스핀스왈로펌</td>\n",
       "      <td>['price', '스핀스왈로펌']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4567388</th>\n",
       "      <td>체인점이 많은 붙임머리 예쁘게 하는 곳 어디인지 알까요</td>\n",
       "      <td>체인점 많 붙임 머리 예쁘 게 곳 인지 알 까요</td>\n",
       "      <td>recommendation</td>\n",
       "      <td>satisfaction</td>\n",
       "      <td>붙임머리</td>\n",
       "      <td>['recommendation', 'satisfaction', '붙임머리']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4567389</th>\n",
       "      <td>살롱드미미 스핀스왈로우 펌 후기 있는가</td>\n",
       "      <td>살 롱드 미미 스핀 스왈로우 펌 후기 있 는가</td>\n",
       "      <td>review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>스핀스왈로우 펌</td>\n",
       "      <td>['review', '스핀스왈로우 펌']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4567390</th>\n",
       "      <td>불편하지 않은 베이비펌 솜씨좋은 헤어샵 궁금해서요</td>\n",
       "      <td>불편 않 베이비펌 솜씨 좋 헤어 샵 궁금 요</td>\n",
       "      <td>recommendation</td>\n",
       "      <td>mood</td>\n",
       "      <td>베이비펌</td>\n",
       "      <td>['recommendation', 'mood', '베이비펌']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4567391</th>\n",
       "      <td>박준뷰티랩 성균관대점 페미닌펌 질문합니다</td>\n",
       "      <td>박준 뷰티 랩 성균관 대점 페미 닌 펌 질문</td>\n",
       "      <td>price</td>\n",
       "      <td>NaN</td>\n",
       "      <td>페미닌펌</td>\n",
       "      <td>['price', '페미닌펌']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               question              question_mecab  \\\n",
       "4567387            아뜰리에7 스핀스왈로펌 비용 어떻지?         뜰 리 스핀 스 왈 로펌 비용 어떻   \n",
       "4567388  체인점이 많은 붙임머리 예쁘게 하는 곳 어디인지 알까요  체인점 많 붙임 머리 예쁘 게 곳 인지 알 까요   \n",
       "4567389           살롱드미미 스핀스왈로우 펌 후기 있는가   살 롱드 미미 스핀 스왈로우 펌 후기 있 는가   \n",
       "4567390     불편하지 않은 베이비펌 솜씨좋은 헤어샵 궁금해서요    불편 않 베이비펌 솜씨 좋 헤어 샵 궁금 요   \n",
       "4567391          박준뷰티랩 성균관대점 페미닌펌 질문합니다    박준 뷰티 랩 성균관 대점 페미 닌 펌 질문   \n",
       "\n",
       "          tag_intention      tag_adjs  tag_hair  \\\n",
       "4567387           price           NaN    스핀스왈로펌   \n",
       "4567388  recommendation  satisfaction      붙임머리   \n",
       "4567389          review           NaN  스핀스왈로우 펌   \n",
       "4567390  recommendation          mood      베이비펌   \n",
       "4567391           price           NaN      페미닌펌   \n",
       "\n",
       "                                           all_tags  \n",
       "4567387                         ['price', '스핀스왈로펌']  \n",
       "4567388  ['recommendation', 'satisfaction', '붙임머리']  \n",
       "4567389                      ['review', '스핀스왈로우 펌']  \n",
       "4567390          ['recommendation', 'mood', '베이비펌']  \n",
       "4567391                           ['price', '페미닌펌']  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_question = df_all_question = pd.read_csv('./all_question_by_mecab_tag.csv'\n",
    "                                                , encoding='utf-8-sig',index_col=0)\n",
    "df_all_question.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 질문의 의도만을 파악(추천/ 문의(가격, 장소, 리뷰)\n",
    "X1_train, X1_test, y1_train, y1_test = \\\n",
    "    train_test_split(df_all_question['question_mecab'].values\n",
    "                    ,df_all_question['tag_intention'].values\n",
    "                    ,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               question              question_mecab  \\\n",
      "2                   밝은 러블리펌 솜씨좋은 미용실 뭐죠        밝 러블리 펌 솜씨 좋 미용실 뭐 죠   \n",
      "4                   균형잡힌 웨이브펌 훌륭한 데 뭔데에        균형 잡힌 웨이브 펌 훌륭 데 뭔 데   \n",
      "5                  봄에 할만한 댄디컷 괜찮은 곳 찾아줘         봄 할 만 댄디 컷 괜찮 곳 찾 줘   \n",
      "19              바로 보이는 포마드컷 지리는 데 궁금합니다            보이 포마드 컷 지리 데 궁금   \n",
      "28            진짜 좋은 포마드컷 예쁘게 하는 데 궁금한데요    진짜 좋 포마드 컷 예쁘 게 데 궁금 한데요   \n",
      "...                                 ...                         ...   \n",
      "4567381        신뢰할만한 S컬펌 실패하지않는 곳 궁금합니다        신뢰 할 만 컬 펌 실패 않 곳 궁금   \n",
      "4567382        여름맞이용 프릴펌 알려주고 싶은 곳 어디예요    여름 맞이 용 프릴 펌 알려 주 싶 곳 예요   \n",
      "4567385       인테리어가 화려한 트위스트펌 지리는 곳 뭐예요    인테리어 화려 트위스트 펌 지리 곳 뭐 예요   \n",
      "4567388  체인점이 많은 붙임머리 예쁘게 하는 곳 어디인지 알까요  체인점 많 붙임 머리 예쁘 게 곳 인지 알 까요   \n",
      "4567390     불편하지 않은 베이비펌 솜씨좋은 헤어샵 궁금해서요    불편 않 베이비펌 솜씨 좋 헤어 샵 궁금 요   \n",
      "\n",
      "          tag_intention      tag_adjs tag_hair  \\\n",
      "2        recommendation          mood     러블리펌   \n",
      "4        recommendation  satisfaction     웨이브펌   \n",
      "5        recommendation           etc      댄디컷   \n",
      "19       recommendation      location     포마드컷   \n",
      "28       recommendation  satisfaction     포마드컷   \n",
      "...                 ...           ...      ...   \n",
      "4567381  recommendation  satisfaction      S컬펌   \n",
      "4567382  recommendation           etc      프릴펌   \n",
      "4567385  recommendation          mood    트위스트펌   \n",
      "4567388  recommendation  satisfaction     붙임머리   \n",
      "4567390  recommendation          mood     베이비펌   \n",
      "\n",
      "                                           all_tags  \n",
      "2                ['recommendation', 'mood', '러블리펌']  \n",
      "4        ['recommendation', 'satisfaction', '웨이브펌']  \n",
      "5                  ['recommendation', 'etc', '댄디컷']  \n",
      "19           ['recommendation', 'location', '포마드컷']  \n",
      "28       ['recommendation', 'satisfaction', '포마드컷']  \n",
      "...                                             ...  \n",
      "4567381   ['recommendation', 'satisfaction', 'S컬펌']  \n",
      "4567382            ['recommendation', 'etc', '프릴펌']  \n",
      "4567385         ['recommendation', 'mood', '트위스트펌']  \n",
      "4567388  ['recommendation', 'satisfaction', '붙임머리']  \n",
      "4567390          ['recommendation', 'mood', '베이비펌']  \n",
      "\n",
      "[1083360 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2) 어떤 특성(ex) 잘하는 곳인지 ,꼼꼼한지, 깨끗한 곳인지...) + 어떤 헤어스타일인지\n",
    "# nan이 있는 tag_feature, tag_hair 컬럼은 nan 값을 제거한 후 따로 저장하여 처리\n",
    "\n",
    "# df_nonan_feature = df_all_question[df_all_question.tag_adjs.notna()]\n",
    "# print(df_nonan_feature) #[1083360 rows x 5 columns]\n",
    "\n",
    "# X2_feature_train, X2_feature_test, y2_feature_train, y2_feature_test =\\\n",
    "# train_test_split(df_all_question['question_mecab'].values\n",
    "#                     ,df_all_question['tag_adjs'].values\n",
    "#                     ,test_size=0.1)\n",
    "\n",
    "# df_nonan_hair = df_all_question[df_all_question.tag_hair.notna()]\n",
    "# print(df_nonan_hair) #[3461952 rows x 5 columns]\n",
    "\n",
    "# X2_hair_train, X2_hair_test, y2_hair_train, y2_hair_test = \\\n",
    "#     train_test_split(df_all_question['question_mecab'].values\n",
    "#                     ,df_all_question['tag_hair'].values\n",
    "#                     ,test_size=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 질문의 의도 + 어떤 특성 + 어떤 헤어스타일인지\n",
    "X3_train, X3_test, y3_train, y3_test = \\\n",
    "    train_test_split(df_all_question['question_mecab'].values\n",
    "                    ,df_all_question['all_tags'].values\n",
    "                    ,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split 된 각각의 train. test 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['영웅 헤어 탈모 처 피 뱅 진짜 리뷰 알려 줄 있 나요', '디보 쉬 헤어 강변 점 보 니 펌 실제 후기 뭡니까',\n",
       "        '봄 할 만 리프 펌 최고 잘 미용실 있 거 죠', ..., '서지희 헤어 자세 주소',\n",
       "        '봄 맞 아이 롱 펌 걱정 없 미용실 어데로', '헤어 리젠트 펌 실제 평점 있'], dtype=object),\n",
       " array(['review', 'review', 'recommendation', ..., 'location',\n",
       "        'recommendation', 'review'], dtype=object))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_train, y1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['가빈 헤어 트위스트 펌 추가 요금 알려 주', '인테리어 잘 언밸런스 컷 싶 미용실 있 는가',\n",
       "        '부 보이 않 펌 추천 가능 곳 있', '월 애 헤어 상세 주소 요', '덕 앤 클리퍼 상세 장소 있'],\n",
       "       dtype=object),\n",
       " array([nan, 'mood', 'satisfaction', nan, nan], dtype=object))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X2_feature_train[10:15], y2_feature_train[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['라브리 헤어 살롱 자양 점 자세 주소 있 나요', '으 니 샵 고데 기 펌 액수 입니까',\n",
       "        '오헤어 신용 산역 점 상세 장소 찾 줘요', ..., '머리 음 일까요',\n",
       "        '누벨바그 헤어 살롱 신도림 점 자세 위치 알려줘',\n",
       "        '박승철 헤어 스투디 마포 점 발 롱 펌 거짓 없 리뷰 여쭤 보 려고요'], dtype=object),\n",
       " array([\"['location']\", \"['price', '고데기펌']\", \"['location']\", ...,\n",
       "        \"['location']\", \"['location']\", \"['review', '발롱펌']\"], dtype=object))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3_train, y3_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_test):\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df = 5\n",
    "                                  ,max_df = 0.85\n",
    "                                  ,ngram_range=(1,3)\n",
    "                                  ,token_pattern='(\\S+)') #공백을 제거한 모든 문자/숫자/특수문자\n",
    "    tfidf_vectorizer.fit(X_train.astype('U'))\n",
    "    X_train_vect = tfidf_vectorizer.transform(X_train)\n",
    "\n",
    "    X_test_vect = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    return X_train_vect, X_test_vect, tfidf_vectorizer.vocabulary_, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-d990b333aca7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX1_train_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX1_test_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_voca_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_vectorizer_1\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtfidf_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX1_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-22013ed1d5ba>\u001b[0m in \u001b[0;36mtfidf_features\u001b[1;34m(X_train, X_test)\u001b[0m\n\u001b[0;32m      5\u001b[0m                                   ,token_pattern='(\\S+)') #공백을 제거한 모든 문자/숫자/특수문자\n\u001b[0;32m      6\u001b[0m     \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'U'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mX_train_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mX_test_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents, copy)\u001b[0m\n\u001b[0;32m   1896\u001b[0m                    \"be removed in 0.24.\")\n\u001b[0;32m   1897\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1898\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1899\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1270\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    219\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# 1) 질문의 의도만을 파악(추천/ 문의(가격, 장소, 리뷰) tfidf 처리\n",
    "%time\n",
    "X1_train_tfidf, X1_test_tfidf, tfidf_voca_1, tfidf_vectorizer_1 = \\\n",
    "    tfidf_features(X1_train, X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 질문의 의도만을 파악(추천/ 문의(가격, 장소, 리뷰) tfidf 처리된 shape\n",
    "X_train_tfidf.shape, X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2) 어떤 특성(ex) 잘하는 곳인지 ,꼼꼼한지, 깨끗한 곳인지...) + 어떤 헤어스타일인지\n",
    "# # tfidf 처리\n",
    "# %%time\n",
    "# X2_feature_tfidf, X1_feature_tfidf, tfidf_voca_2_feature, tfidf_vectorizer_2_feature = \\\n",
    "#     tfidf_features(X2_feature_train, X2_feature_test)\n",
    "# X2_hair_tfidf, X1_hair_tfidf, tfidf_voca_2_hair, tfidf_vectorizer_2_hair = \\\n",
    "#     tfidf_features(X2_hair_train, X2_hair_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2) 어떤 특성(ex) 잘하는 곳인지 ,꼼꼼한지, 깨끗한 곳인지...) + 어떤 헤어스타일인지 tfidf 처리된 shape\n",
    "# X2_feature_train_tfidf.shape, X2_feature_test_tfidf.shape\n",
    "# X2_hair_train_tfidf.shape, X2_hair_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 질문의 의도 + 어떤 특성 + 어떤 헤어스타일인지\n",
    "%%time\n",
    "X3_train_tfidf, X3_test_tfidf, tfidf_voca_3, tfidf_vectorizer_3 = \\\n",
    "    tfidf_features(X3_train, X3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 질문의 의도 + 어떤 특성 + 어떤 헤어스타일인지 tfidf 처리된 shape\n",
    "X3_train_tfidf.shape, X3_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_voca_1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_voca_2_feature[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_voca_2_hair[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_voca_3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf vect 처리된 형태소들을 이후 처리하기 쉽게 {벡터화 수치:형태소} 꼴로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_reversed_voca_1=\\\n",
    "    {vect_i : word for word, vect_i in tfidf_voca_1.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_reversed_voca_2_feature=\\\n",
    "    {vect_i : word for word, vect_i in tfidf_voca_2_feature.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_reversed_voca_2_hair=\\\n",
    "    {vect_i : word for word, vect_i in tfidf_voca_2_hair.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_reversed_voca_3=\\\n",
    "    {vect_i : word for word, vect_i in tfidf_voca_3.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목적별로 작성된 tfidf 벡터화 데이터 pickle 로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X1_train_tfidf.pickle','wb') as f:\n",
    "    pickle.dump(X1_train_tfidf, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('X1_test_tfidf.pickle','wb') as f:\n",
    "    pickle.dump(X1_test_tfidf, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('X2_feature_train_tfidf.pickle','wb') as f:\n",
    "    pickle.dump(X2_feature_train_tfidf, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('X2_feature_test_tfidf.pickle','wb') as f:\n",
    "    pickle.dump(X2_feature_test_tfidf, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('X2_hair_train_tfidf.pickle','wb') as f:\n",
    "    pickle.dump(X2_hair_train_tfidf, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('X2_hair_test_tfidf.pickle','wb') as f:\n",
    "    pickle.dump(X2_hair_test_tfidf, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('X3_train_tfidf.pickle','wb') as f:\n",
    "    pickle.dump(X3_train_tfidf, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('X3_test_tfidf.pickle','wb') as f:\n",
    "    pickle.dump(X3_test_tfidf, f, pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_voca_1.pickle','wb') as f:\n",
    "    pickle.dump(tfidf_voca_1, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('tfidf_voca_2_feature.pickle','wb') as f:\n",
    "    pickle.dump(tfidf_voca_2_feature, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('tfidf_voca_2_hair.pickle','wb') as f:\n",
    "    pickle.dump(tfidf_voca_2_hair, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('tfidf_voca_3.pickle','wb') as f:\n",
    "    pickle.dump(tfidf_voca_3, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('tfidf_reversed_voca_1.pickle','wb') as f:\n",
    "    pickle.dump(tfidf_reversed_voca_1, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('tfidf_reversed_voca_2_feature.pickle','wb') as f:\n",
    "    pickle.dump(tfidf_reversed_voca_2_feature, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('tfidf_reversed_voca_2_hair.pickle','wb') as f:\n",
    "    pickle.dump(tfidf_reversed_voca_2_hair, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('tfidf_reversed_voca_3.pickle','wb') as f:\n",
    "    pickle.dump(tfidf_reversed_voca_3, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# y_train 데이터의 tag 부분 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-5-2b5a75ef0542>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-2b5a75ef0542>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    for tag in y1_train:\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#tag가 train 데이터에서 몇 번 나오는지 count 해서 딕셔너리에 저장한다\n",
    "tags_counts_1 = {}\n",
    "tags_counts_2_feature = {}\n",
    "tags_counus_2_hair = {}\n",
    "tags_counts_3 = {}\n",
    "\n",
    "for tag in y1_train:\n",
    "    if tag in tag_counts_1.keys():\n",
    "        tags_counts_1[tag] += 1\n",
    "    else:\n",
    "        tags_counts_1[tag] = 1\n",
    "\n",
    "for tag in y2_feature_train:\n",
    "    if tag in tag_counts_2_feature.keys():\n",
    "        tags_counts_2_feature[tag] += 1\n",
    "    else:\n",
    "        tags_counts_2_feature[tag] = 1\n",
    "\n",
    "for tag in y2_hair_train:\n",
    "    if tag in tag_counts_2_hair.keys():\n",
    "        tags_counts_2_hair[tag] += 1\n",
    "    else:\n",
    "        tags_counts_2_hair[tag] = 1\n",
    "\n",
    "# y3_train은 tag가 여러개 저장(리스트 형태)되어 있으므로 이중 for문을 사용\n",
    "for tags in y3_train:\n",
    "    for tag in tags:\n",
    "        if tag in tags_counts_3.keys():\n",
    "            tag_counts_3[tag] += 1\n",
    "        else:\n",
    "            tags_counts_3[tag] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 태그 부분 transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelBinaizer 생성\n",
    "- 단일 태그가 있는 데이터는 LabelBinaizer를 사용하여 처리한다.\n",
    "- 단일 태그가 있는 데이터 => y1, y2_feature, y2_hair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y1_train, test data fit, transfrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_train.shape, y1_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y1_train 데이터의 경우 y1_train의 태그로 LabelBinaizer를 fit 함\n",
    "lb_1 = LabelBinarizer()\n",
    "lb_1.fit(y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y1_train으로 fir한 lb_1 class에는 y1_train의 태그 내용이 있음\n",
    "lb_1.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform 시 나올 결과 미리 sample로 확인해 보기\n",
    "y1_train[:5], lb_1.transform(y1_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_train = lb_1.transform(y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_test = lb_1.transform(y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_train.shape, y1_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y2_train, test data fit, transfrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_feature_train.shape, y2_feature_test.shape #fit, trans 전 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_2_feature = LabelBinarizer()\n",
    "y2_feature_train = lb_2_feature.fit_transform(y2_feature_train) #fit, transform 동시에\n",
    "y2_feature_test = lb_2_feature.transform(y2_feature_test) #y2_feature_train로 fit된것에 transform 처리만\n",
    "\n",
    "lb_2_feature.classes #lb_2_feature class 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_feature_train.shape, y2_feature_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform 한 결과 sample로 확인해 보기\n",
    "y2_feature_train[:5], y2_feature_test[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_hair_train.shape, y2_hair_test.shape #fit, trans 전 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_2_hair = LabelBinarizer()\n",
    "y2_hair_train = lb_2_hair.fit_transform(y2_hair_train) #fit, transform 동시에\n",
    "y2_hair_test = lb_2_hair.transform(y2_hair_test) #y2_feature_train로 fit된것에 transform 처리만\n",
    "\n",
    "lb_2_hair.classes #lb_2_feature class 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_hair_train.shape, y2_hair_test.shape #fit, trans 전 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform 한 결과 sample로 확인해 보기\n",
    "y2_hair_train[:5], y2_hair_test[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tag가 여러개인 y3의 경우 => MultiLabelBinarizer 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_3 = MultiLabelBinarizer(classes=sorted(tags_counts_3.keys()))\n",
    "mlb_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform 미리보기\n",
    "y3_train[0], mlb_3.fit_transform(y3_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##transform 미리보기\n",
    "y3_train[5], mlb_3.fit_transform(y3_train)[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('벡터화 전 shape:', 'train:', y3_train.shape, 'test': y3_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y3_train, y3_test 벡터화\n",
    "y3_train = mlb_3.fit_transform(y3_train)\n",
    "y3_test = mlb_3.fit_transform(y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('벡터화 후 shape:', 'train:', y3_train.shape, 'test': y3_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pickle 로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y1_train.pickle','wb') as f:\n",
    "    pickle.dump(y1_train, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('y1_test.pickle','wb') as f:\n",
    "    pickle.dump(y1_test, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('y2_feature_train.pickle','wb') as f:\n",
    "    pickle.dump(y2_feature_train, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('y2_feature_test.pickle','wb') as f:\n",
    "    pickle.dump(X2_feature_test, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('y2_hair_train.pickle','wb') as f:\n",
    "    pickle.dump(y2_hair_train, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('y2_hair_test.pickle','wb') as f:\n",
    "    pickle.dump(y2_hair_test, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('y3_train.pickle','wb') as f:\n",
    "    pickle.dump(y3_train, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('y3_test.pickle','wb') as f:\n",
    "    pickle.dump(y3_test, f, pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lb_1.pickle','wb') as f:\n",
    "    pickle.dump(lb_1, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('lb_2_feature.pickle','wb') as f:\n",
    "    pickle.dump(lb_2_feature, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('lb_2_hair.pickle','wb') as f:\n",
    "    pickle.dump(lb_2_hair, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('mlb_3.pickle','wb') as f:\n",
    "    pickle.dump(mlb_3, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
